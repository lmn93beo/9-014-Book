{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vectors\n",
    "\n",
    "### Definitions and examples\n",
    "\n",
    "At a high level, a vector is a collection of numbers that together represents some object. For instance, imagine we decide to collect information about patients. Each patient can be represented by three numbers, namely, their age, weight (in kg) and height (in cm). So Alice might represented by the vector $\\textbf{a} = (23, 70, 180)$, Bob by the vector $\\textbf{b} = (31, 80, 185)$.\n",
    "\n",
    "In neuroscience, a common example of a vector is the state of an ensemble of $n$ neurons. In this context, each state is represented by $n$ numbers $(a_1, a_2, ..., a_n)$ where each number represents an activity of a neuron.\n",
    "\n",
    "Geometrically, each vector can be thought of as an arrow starting from the origin and ending at the coordinates specified by the components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition and scalar multiplication\n",
    "\n",
    "To add two vectors, we add the components individually. To multiply a vector by a scalar (a single number), we multiply each number of the vector by that scalar.\n",
    "\n",
    "*Example:* Using the two vectors $\\textbf{a} = (23, 70, 180)$ and $\\textbf{b} = (31, 80, 185)$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\textbf{a} + \\textbf{b} &= (54, 150, 365)\\\\\n",
    "\\textbf{a} - \\textbf{b} &= (-8, -10, -5)\\\\\n",
    "2\\textbf{a} &= (46, 140, 360)\\\\\n",
    "-3\\textbf{b} &= (-93, -240, -555)\\\\\n",
    "2\\textbf{a} - 3\\textbf{b} &= (-47, -100, -195)\n",
    "\\end{align*}\n",
    "\n",
    "Each of these two operations has a geometric interpretation. To add two vectors $\\textbf{a}$ and $\\textbf{b}$, we join them head-to-tail as shown below. The sum $\\textbf{c} = \\textbf{a} + \\textbf{b}$ is given by the resulting arrow.\n",
    "\n",
    "Scalar multiplication corresponds to a lengthening or shortening of a vector by the appropriate scale factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norm\n",
    "\n",
    "The norm of a vector is a number that represents how long that vector is. Given a vector $\\textbf{v} = (v_x, v_y)$, we can use the Pythagoras' theorem to calculate its length (see diagram). The length of $\\textbf{v}$ is denoted by $||\\textbf{v}||$ and is given by\n",
    "\n",
    "$$||\\textbf{v}|| = \\sqrt{v_x^2 + v_y^2}$$\n",
    "\n",
    "For vectors in higher dimensions, we get a similar result. Consider a vector $\\textbf{u} = (u_1, u_2, ..., u_n)$, then\n",
    "\n",
    "$$||\\textbf{u}|| = \\sqrt{u_1^2 + u_2^2 +...+u_n^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot product\n",
    "\n",
    "Before going into why the dot product is useful, let us see what it is. The dot product of two vectors is the sum of their component-wise products.\n",
    "\n",
    "$$\\textbf{a} \\ .\\ \\textbf{b} = a_1b_1 + a_2b_2 + ... + a_nb_n$$\n",
    "\n",
    "We will discuss three applications of the dot product\n",
    "\n",
    "(i) *Angles*\n",
    "\n",
    "Imagine two vectors $v_1$ and $v_2$ in the 2D plane. We want to find the angle between them. To do that, we can make use of a cool result in trigonometry\n",
    "\n",
    "$$v_1\\ . \\ v_2 = ||v_1||||v_2||\\cos \\theta$$\n",
    "\n",
    "where $\\theta$ is the angle between the two vectors. Therefore, the angle between $v_1$ and $v_2$ is given by\n",
    "\n",
    "$$\\cos\\theta = \\frac{v_1\\ . \\ v_2}{||v_1||||v_2||}$$\n",
    "\n",
    "(ii) *Synaptic integration*\n",
    "\n",
    "Here we discuss a very basic formulation of a neural network. A neural network is composed of units arranged in layers. We can number the layers 1, 2, ..., $n$. In each layer, neurons send connections to the next layer. Each neuron has an activity denoted by a number.\n",
    "\n",
    "Now we consider only a single neuron in the neural network. Let $x$ be the activity of this neuron. Assume this neuron receives inputs from $n$ neurons in the previous layer $v_1, v_2, ..., v_n$, with corresponding synaptic weights $w_1, w_2, ..., w_n$. Then the activity of this neuron is the sum of all the inputs weighted by the corresponding synaptic weight.\n",
    "\n",
    "$$x = w_1v_1 + w_2v_2 + ... + w_nv_n$$\n",
    "\n",
    "Writing this as a dot product\n",
    "\n",
    "$$x = \\textbf{w} \\ . \\ \\textbf{v}$$\n",
    "\n",
    "where $\\textbf{w} = (w_1, ..., w_n)$ is the vector off all the weights and $\\textbf{v} = (v_1, ..., v_n)$ is the vector of all the inputs.\n",
    "\n",
    "(iii) *Some interesting results*\n",
    "\n",
    "* The norm of a vector is square root of the dot product with itself\n",
    "\t\n",
    "\t$$||v|| = \\sqrt{\\textbf{v} \\ . \\ \\textbf{v}}$$\n",
    "\t\n",
    "* Two vectors $\\textbf{a}$ and $\\textbf{b}$ are orthogonal (perpendicular) if and only if $\\textbf{a} \\ . \\ \\textbf{b} = 0$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(iii) *Projection*\n",
    "\n",
    "An important concept that we will discuss many times in this chapter is projection. Let's try to build some intuition in the 2D plane.\n",
    "\n",
    "Let's say we have a vector $\\textbf{v} = (3, 5)$. As shown in figure 1, the *projection* of $\\textbf{v}$ onto the $x$-axis has length 3 and the *projection* of $\\textbf{v}$ onto the $y$-axis has length 5. We can think of projections as the 'shadows' cast by $\\textbf{v}$ on the relevant axes.\n",
    "\n",
    "What if we want to project $\\textbf{v}$ onto some arbitrary axis, as shown in figure ...? As explained in class, the formula we need to find the dot product of $\\textbf{v}$ with the unit vector along that axis.\n",
    "\n",
    "$$x = \\frac{\\textbf{v} \\ . \\ \\textbf{w}}{||\\textbf{w||}}$$\n",
    "\n",
    "For the special case of projecting onto the x- and y-axes, we get\n",
    "\n",
    "$$v_x = ||v||\\cos\\theta$$\n",
    "\n",
    "$$v_y = ||v||\\sin\\theta$$\n",
    "\n",
    "*Example:* Find the projection of vector $\\textbf{v} = (-1, 2)$ onto the unit vector $\\textbf{u} = \\left(\\frac{1}{2}, \\frac{\\sqrt{3}}{2}\\right)$\n",
    "\n",
    "** *Solution* **: \n",
    "\n",
    "The length of that projection is given by $\\textbf{v}. \\textbf{u} = (-1) \\frac{1}{2} + 2 \\frac{\\sqrt{3}}{2} = \\sqrt{3} - \\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
